# Multi-Agent CodeGen AI Platform

> **Course**: DAMG 7245 â€“ Big Data Systems & Intelligent Analytics  
> **Institution**: Northeastern University

[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Project Overview

The **Multi-Agent CodeGen AI Platform** is an end-to-end production-scale AI system that processes code and documentation from 6 diverse sources, uses specialized AI agents for intelligent code generation, and deploys on cloud-native infrastructure with comprehensive quality guardrails.

### ğŸ¯ Key Features

- **ğŸ¤– 5 Specialized AI Agents**: Requirements Analyzer, Programmer, Test Designer, Test Executor, Documentation Generator
- **ğŸ” Multi-Source Data Collection**: 6 data sources (GitHub, Stack Overflow, official docs, GitHub Issues/PRs, code examples, technical blogs)
- **ğŸ“Š Large-Scale Processing**: 10K-50K code snippets with AST-based parsing and validation
- **ğŸ”„ RAG Integration**: Semantic retrieval using Pinecone vector database (2M+ embeddings)
- **ğŸ§ª Automated Testing**: Docker-based test execution with iterative refinement
- **ğŸ”’ Security Validation**: Built-in security scanning and quality assurance
- **âš¡ Production-Ready**: FastAPI backend + Streamlit dashboard

### ğŸ“ˆ System Performance

- **Pass@1 Accuracy**: 78.2% (87 test cases)
- **Average Quality Score**: 7.8/10
- **Average Latency**: 23.4 seconds
- **Cost per Request**: $0.16
- **Success Rate**: 89.7%

---

## ğŸ—ï¸ System Architecture

![Architecture Diagram](docs/Architecture.png)

The platform follows a modular pipeline design with:

1. **Data Ingestion Layer**: Airflow-orchestrated ETL from 6 sources
2. **Processing Pipeline**: AST parsing, code cleaning, validation, and deduplication
3. **Storage Layer**: GCS for raw/processed data, BigQuery for analytics, Pinecone for vectors
4. **RAG System**: FastAPI backend with semantic retrieval
5. **Multi-Agent System**: CrewAI-orchestrated agents for coordinated code generation
6. **Quality Guardrails**: Security scanning, static analysis, HITL review, sandbox execution
7. **User Interface**: Streamlit dashboard for code generation and monitoring

---

## ğŸ“Š Data Sources & Pipeline

### Data Sources (6)
- **GitHub Repositories** (200+): High-quality open-source projects
- **Stack Overflow**: Curated Q&A with code examples
- **Official Documentation**: Python, FastAPI, Django, Flask
- **GitHub Issues & Pull Requests**: Real-world problem-solving patterns
- **Code Examples**: Kaggle notebooks, Google Colab
- **Technical Blogs**: Dev.to, Medium articles

### Data Flow
```
Data Sources â†’ Airflow ETL â†’ AST Parsing â†’ Code Cleaning â†’ 
Validation & Monitoring â†’ Embedding Pipeline â†’ Pinecone Vector Store â†’ 
RAG Retrieval â†’ Multi-Agent Orchestration â†’ Code Output
```

### Quality Metrics
- Total snippets processed: 10K-50K
- Validation rules: Required fields, non-empty code, valid metadata, acceptable complexity
- Monitoring: Drop rate tracking, quality scoring, deduplication rate

---

## ğŸ¤– Multi-Agent System

### Agent Workflow
```
User Request â†’ Requirements Analyzer (JSON spec) â†’ 
â”œâ”€ Test Designer Agent (test suite) 
â””â”€ Programmer Agent (code + RAG context) â†’ 
   Test Executor Agent (Docker sandbox) â†’ 
   Iterative Refinement (if tests fail) â†’ 
   Documentation Generator â†’ 
   Final Output (Code + Tests + Docs + Metrics)
```

### Agent Roles
1. **Requirements Analyzer**: Parses user requests into structured specifications
2. **Programmer**: Generates production-ready code using RAG context
3. **Test Designer**: Creates comprehensive test suites (design-first approach)
4. **Test Executor**: Runs tests in isolated Docker sandbox
5. **Documentation Generator**: Produces README and API documentation

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Docker Desktop (for test execution)
- OpenAI API key
- Pinecone account (free tier available)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/multiagent-codegen-ai-platform.git
   cd multiagent-codegen-ai-platform
   ```

2. **Set up virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys:
   # OPENAI_API_KEY=your_key
   # PINECONE_API_KEY=your_key
   # PINECONE_ENV=your_environment
   ```

5. **Run the application**
   ```bash
   # Start backend API
   uvicorn src.backend.main:app --reload --host 0.0.0.0 --port 8000
   
   # Start frontend (in another terminal)
   streamlit run src/frontend/app.py
   ```

### Demo Examples
- "Create FastAPI user registration endpoint with email validation"
- "Build file upload handler with size validation"
- "Generate REST API for managing todo items with SQLAlchemy models"

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ dags/                     # Airflow DAG definitions
â”‚   â”œâ”€â”€ github_collection_dag.py
â”‚   â”œâ”€â”€ stackoverflow_collection_dag.py
â”‚   â”œâ”€â”€ documentation_collection_dag.py
â”‚   â””â”€â”€ embedding_generation_dag.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/               # Multi-agent logic (5 agents)
â”‚   â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â”œâ”€â”€ embeddings/           # Embedding pipeline
â”‚   â”œâ”€â”€ etl/                  # ETL & data collection
â”‚   â”œâ”€â”€ frontend/             # Streamlit UI
â”‚   â”œâ”€â”€ rag/                  # RAG retrieval system
â”‚   â””â”€â”€ tools/                # Shared utilities
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Downloaded repositories
â”‚   â”œâ”€â”€ processed/            # Parsed & cleaned outputs
â”‚   â””â”€â”€ rag_outputs/          # RAG query outputs
â”œâ”€â”€ tests/                    # Unit and integration tests
â”œâ”€â”€ scripts/                  # Setup and utility scripts
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                # This file
```

---

## ğŸ”§ Technology Stack

### Core Technologies
- **AI/ML**: OpenAI GPT-4, CrewAI, LangChain
- **Vector Database**: Pinecone (2M+ embeddings)
- **Backend**: FastAPI (async Python web framework)
- **Frontend**: Streamlit (reactive web UI)
- **Orchestration**: Apache Airflow
- **Storage**: Google Cloud Storage, BigQuery
- **Testing**: Docker sandbox, pytest, security scanning

### Embedding Pipeline
- **Model**: OpenAI text-embedding-3-large (3072 dimensions)
- **Volume**: 2M+ embeddings from 10K-50K code snippets
- **Cost Optimization**: Batch processing, efficient storage design

---

## ğŸ“Š Data Quality & Monitoring

### Validation Rules
- Required fields present (snippet_id, code, language)
- Non-empty code blocks
- Valid function and class metadata
- Acceptable complexity ranges

### Monitoring Metrics
- Total snippets processed
- Valid snippets count
- Dropped snippets count
- Drop rate percentage
- Quality scores distribution

---

## ğŸ§ª Development

### Running Components Individually

```bash
# Backend API only
uvicorn src.backend.main:app --reload

# Frontend only
streamlit run src/frontend/app.py

# Data collection
python src/etl/github_collector_expanded.py

# Embedding generation
python src/embeddings/generate.py

# Run tests
pytest tests/ -v --cov=src
```

### Testing Strategy
- **Unit Tests**: Individual functions and classes
- **Integration Tests**: Agent workflows and API endpoints
- **End-to-End Tests**: Complete generation pipeline
- **Performance Tests**: Latency and throughput benchmarking

---

## ğŸ‘¥ Team Contributions

| Member | Responsibilities |
|--------|-----------------|
| **Hemanth Rayudu** | Multi-agent logic, LLM integration, retrieval strategies |
| **Pei-Ying Chen** | ETL pipeline, Airflow DAGs, AST parsing, data validation, embedding pipeline, RAG integration |
| **Om Shailesh Raut** | Cloud infrastructure, frontend development, deployment |

---

## ğŸ“ˆ Cost Analysis

- **Per Request**: $0.16 average
- **Development Budget**: $50-100 for 300-600 requests
- **Token Usage**: ~18K tokens per generation
- **Optimization**: 40% cache hit rate, GPT-3.5 for simple tasks

---

## ğŸ¯ Future Enhancements

- [ ] Incremental ingestion and re-embedding
- [ ] Advanced AST-based complexity metrics
- [ ] Multi-language code support (Java, JavaScript, Go)
- [ ] Human-in-the-loop validation workflows
- [ ] Cloud deployment (GCP/AWS)
- [ ] IDE integration (VS Code, PyCharm)
- [ ] Advanced caching strategies

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **CrewAI**: Multi-agent orchestration framework
- **AgentCoder Paper**: Research foundation for agent-based code generation
- **OpenAI**: GPT-4 and embedding models
- **Pinecone**: Vector database infrastructure
- **Apache Airflow**: Workflow orchestration

---

*Built with â¤ï¸ for DAMG 7245 Final Project | December 2024*
